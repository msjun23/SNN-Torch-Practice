{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7 - Neuromorphic Datasets with Tonic + snnTroch\n",
    "\n",
    "## Learn how to\n",
    "- load neuromorphic datasets using Tonic\n",
    "- make use of chaching to speed up dataloading\n",
    "- train a ConvSNN with the N-MNIST dataset\n",
    "\n",
    "> Tonin 라이브러리를 활용하여 Neuromorphic 데이터셋을 로드하고 학습하는 과정에 대해 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Tonic to load Neuromorphic datasets\n",
    "\n",
    "Tonic 라이브러리는 Pytorch vision과 비슷하게 작동한다. Tonic을 사용하여 N-MNIST와 같은 neuromorphic 데이터셋을 로드할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/python_ws/SNN-Torch-Practice/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 0, 30,   1213, 1) ( 9, 18,   4044, 0) (30, 15,   4104, 1) ...\n",
      " ( 9, 17, 301658, 0) (18,  8, 305112, 1) (19, 26, 309976, 1)]\n",
      "<class 'numpy.ndarray'>\n",
      "(3913,)\n",
      "<class 'tonic.datasets.nmnist.NMNIST'> 60000 <class 'tuple'> (array([( 0, 30,   1213, 1), ( 9, 18,   4044, 0), (30, 15,   4104, 1), ...,\n",
      "       ( 9, 17, 301658, 0), (18,  8, 305112, 1), (19, 26, 309976, 1)],\n",
      "      dtype=[('x', '<i8'), ('y', '<i8'), ('t', '<i8'), ('p', '<i8')]), 9)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tonic\n",
    "import numpy as np\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)       # 9~0, total 60,000\n",
    "events, target = dataset[0]\n",
    "print(events)       # [(x,y,timestamp,polarity), (x,y,timestamp,polarity), ...]\n",
    "print(type(events))\n",
    "print(np.shape(events))\n",
    "print(type(dataset), len(dataset), type(dataset[0]), dataset[0])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 N-MNIST 데이터셋을 로드할 수 있다. MNIST와 동일하게 총 6만장의 이미지가 있으며, 각 이미지들을 $ms$ 단위의 event들로 재생성한 것이다. events는 수천개 이벤트들의 리스트이고, 하나의 이벤트는 (x, y, timestamp, polarity)로 표현된다. *polarity* 는 $+1\\ or\\ -1$ 이며, 빛의 밝기가 밝아지거나 어두워지거나 둘 중 하나이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Transformation\n",
    "위와 같은 raw 데이터를 네트워크의 입력으로 사용할 수 없다. Pytorch dataset과 비슷하게 텐서로의 변환 과정이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time-window\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000), \n",
    "                                      transforms.ToFrame(sensor_size=sensor_size, time_window=1000)])\n",
    "\n",
    "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
    "testset  = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fast data loading via caching\n",
    "데이터 캐싱을 통해 데이터를 로드하는데 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467 ms ± 4.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 467 ms ± 4.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_sample_simple():\n",
    "    for i in range(100):\n",
    "        events, target = trainset[i]\n",
    "        \n",
    "%timeit -o load_sample_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch DataLoader와 디스크 캐싱을 통해 위 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14558/1546661115.py:4: DeprecationWarning: CachedDataset is deprecated and will be removed in a future release. It currently points to DiskCachedDataset to distinguish it from MemoryCachedDataset. Documentation available under https://tonic.readthedocs.io/en/latest/reference/data_classes.html#caching\n",
      "  cached_trainset   = CachedDataset(trainset, cache_path='./cache/nmist/train')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 ms ± 4.51 ms per loop (mean ± std. dev. of 20 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 221 ms ± 4.51 ms per loop (mean ± std. dev. of 20 runs, 1 loop each)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import CachedDataset\n",
    "\n",
    "cached_trainset   = CachedDataset(trainset, cache_path='./cache/nmist/train')\n",
    "cached_dataloader = DataLoader(cached_trainset)\n",
    "\n",
    "def load_sample_cached():\n",
    "    for i, (events, target) in enumerate(iter(cached_dataloader)):\n",
    "        if i > 99: break\n",
    "        \n",
    "%timeit -o -r 20 load_sample_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Even faster dataloading via batching\n",
    "\n",
    "데이터 로딩 시간을 줄였다. 이제 GPU를 더욱 효율적으로 사용하기 위해 batching 한다.\n",
    "\n",
    "각각의 이벤트들은 기록 시간이 모두 다르기 때문에, **tonic.collation.PadTensors()** 함수를 사용하여 batch 내부의 모든 데이터들이 같은 디멘션을 가질 수 있도록해준다. 기록 시간이 짧은 것들을 pad out하는 방식을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79 ms ± 5.87 µs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 3.79 ms ± 5.87 µs per loop (mean ± std. dev. of 10 runs, 100 loops each)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())\n",
    "\n",
    "def load_sample_batched():\n",
    "    events, target = next(iter(cached_dataloader))\n",
    "    \n",
    "%timeit -o -r 10 load_sample_batched()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training our network using frames created from events\n",
    "\n",
    "Training data에 augmentation을 적용하고자 한다. 캐싱된 데이터셋에서 가져온 샘플들은 프레임(텐서)이기 때문에, torchvision을 적용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14558/1859081736.py:7: DeprecationWarning: CachedDataset is deprecated and will be removed in a future release. It currently points to DiskCachedDataset to distinguish it from MemoryCachedDataset. Documentation available under https://tonic.readthedocs.io/en/latest/reference/data_classes.html#caching\n",
      "  cached_trainset = CachedDataset(trainset, transform=transform, cache_path='./cache/nmist/train')\n",
      "/tmp/ipykernel_14558/1859081736.py:9: DeprecationWarning: CachedDataset is deprecated and will be removed in a future release. It currently points to DiskCachedDataset to distinguish it from MemoryCachedDataset. Documentation available under https://tonic.readthedocs.io/en/latest/reference/data_classes.html#caching\n",
      "  cached_testset  = CachedDataset(testset, cache_path='./cache/nmist/test')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = tonic.transforms.Compose([torch.from_numpy, \n",
    "                                      torchvision.transforms.RandomRotation([-10, 10])])\n",
    "\n",
    "cached_trainset = CachedDataset(trainset, transform=transform, cache_path='./cache/nmist/train')\n",
    "# No augmentation for the testset\n",
    "cached_testset  = CachedDataset(testset, cache_path='./cache/nmist/test')\n",
    "\n",
    "batch_size  = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)\n",
    "testloader  = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 미니배치는 다음과 같은 파일 포맷을 가진다.\n",
    "\n",
    "> (batch_size, time_steps, channels, height, width)\n",
    "\n",
    "여기서 *time_steps*는 미니배치에 있는 데이터 중 이벤트의 길이가 가장 긴 데이터에 맞춰진다. 그리고 다른 데이터들의 모자란 부붙은 0으로 채워진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 311, 2, 34, 34])\n"
     ]
    }
   ],
   "source": [
    "event_tensor, target = next(iter(trainloader))\n",
    "print(event_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Defining our network\n",
    "\n",
    "Pytorch와 snnTorch를 사용하여 ConvSNN을 설계한다. 아키텍처는 다음과 같다.\n",
    "\n",
    "> 12C5-MP2-32C5-MP2-800FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate, utils\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Neuron and simulation params\n",
    "spike_grad = surrogate.fast_sigmoid(slope=75)\n",
    "beta       = 0.5\n",
    "\n",
    "# Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(2, 12, 5), \n",
    "                    nn.MaxPool2d(2), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True), \n",
    "                    nn.Conv2d(12, 32, 5), \n",
    "                    nn.MaxPool2d(2), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True), \n",
    "                    nn.Flatten(), \n",
    "                    nn.Linear(32*5*5, 10), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not return membrane as we don't need it\n",
    "\n",
    "def forward_pass(net, data):\n",
    "    spk_rec = []\n",
    "    utils.reset(net)        # Reset hidden states for all LIF neurons in net\n",
    "    \n",
    "    print(data.shape)\n",
    "    data = data.transpose(0,1)\n",
    "    print(data.shape)\n",
    "    for step in range(data.size(0)):        # data.size(0) = num of time steps\n",
    "        spk_out, mem_out = net(data[step])\n",
    "        spk_rec.append(spk_out)\n",
    "        \n",
    "    return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 310, 2, 34, 34])\n",
      "torch.Size([310, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 31.00\n",
      "Accuracy: 3.91%\n",
      "\n",
      "torch.Size([128, 311, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 30.90\n",
      "Accuracy: 9.38%\n",
      "\n",
      "torch.Size([128, 311, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 30.90\n",
      "Accuracy: 6.25%\n",
      "\n",
      "torch.Size([128, 312, 2, 34, 34])\n",
      "torch.Size([312, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 30.96\n",
      "Accuracy: 8.59%\n",
      "\n",
      "torch.Size([128, 311, 2, 34, 34])\n",
      "torch.Size([311, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 30.90\n",
      "Accuracy: 9.38%\n",
      "\n",
      "torch.Size([128, 312, 2, 34, 34])\n",
      "torch.Size([312, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 30.96\n",
      "Accuracy: 9.38%\n",
      "\n",
      "torch.Size([128, 313, 2, 34, 34])\n",
      "torch.Size([313, 128, 2, 34, 34])\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 31.02\n",
      "Accuracy: 13.28%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "num_epochs = 1\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(acc_hist)\n",
    "plt.title(\"Train Set Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 127       # At minibatch\n",
    "\n",
    "print(f\"The target label is: {targets[idx]}\")\n",
    "\n",
    "tonic.utils.plot_event_grid(data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('SNN-Torch-Practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "683a7b102b596254c395704dcf4840313938b4a008d6dc868d87bbb00d08873e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
