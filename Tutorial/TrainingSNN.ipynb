{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 - Training Spiking Neural Networks with snnTorch\n",
    "\n",
    "## Learn how to\n",
    "- implement spiking neurons as a recurrent network\n",
    "- understand backpropagation through time, and the associated challenges in SNNs such as the non-differentiability of spikes\n",
    "- train a FC network on the static MNIST dataset\n",
    "\n",
    "> Spiking neuron을 사용하여 네트워크를 구현하고, backpropagation과 실제 데이터 학습에 대해 알아본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "from snntorch import spikeplot as splt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A Recurrent representation of SNNs\n",
    "\n",
    "이제껏 다룬 SNN은 이산 시간에서 다뤄졌고, 이전 출력의 영향을 받는다. 전체적인 computational graph를 그려보면 아래와 같다.\n",
    "\n",
    "![unrolled2](./images/unrolled_2.png)\n",
    "\n",
    "> $U[t+1]=\\underbrace{\\beta U[t]}_{decay}+\\underbrace{I_{syn}[t+1]}_{Input}-\\underbrace{R[t]}_{reset}$\n",
    ">\n",
    "> $S[t]=\\left\\{\\begin{matrix} 1,\\ if U(t) > U_{thr} \\\\ 0,\\ \\ \\ \\ \\ \\ \\ otherwise \\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Non-Differentiability of spikes\n",
    "\n",
    "## 2.1 Training using the Backprop algorithm\n",
    "\n",
    "위 식에서 *Reset* 텀의 $S[t]$ 는 다음과 같이 *Unit step function* 으로 표현할 수 있다.\n",
    "\n",
    "> $S[t]=\\Theta(U[t]-U_{thr})$\n",
    "\n",
    "이와 같은 *step function* 은 backprop 과정에서 아래와 같이 미분이 불가능해지는 문제를 발생시킨다.\n",
    "\n",
    "![non-diff](./images/non-diff.png)\n",
    "\n",
    "> $\\frac{\\partial L}{\\partial W}=\\frac{\\partial L}{\\partial S}\\underbrace{\\frac{\\partial S}{\\partial U}}_{\\{0,\\infty\\}}\\frac{\\partial U}{\\partial I}\\frac{\\partial I}{\\partial W}$\n",
    "\n",
    "위와 같은 chain rule로 인해 대부분의 경우 gradient가 0이 되고, $U_{thr}=0$ 인 경우에는 무한대가 된다. 기울기가 거의 항상 0으로 유지되면(U가 정확히 threshold에 위치할 때는 무한대로 발산) 어떠한 학습도 일어날 수 없고, 이는 **Dead Neuron Problem** 으로 알려져있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Overcome the Dead Neuron Problem\n",
    "\n",
    "위와 같은 Dead Neuron Problem을 방지하기 위해서 $\\frac{\\partial S}{\\partial U}$ 텀을 역전파가 이어질 수 있도록 임의의 $\\frac{\\partial \\tilde{S}}{\\partial U}$ 로 변경한다. 이러한 근사치는 꽤나 강인한 방법으로 알려져 있고, 이를 **Surrogate Gradient** approach 라고 한다.\n",
    "\n",
    "이번 튜토리얼에서는 간단하게 하기 위해, $\\frac{\\partial \\tilde{S}}{\\partial U}=S$ 로 정의한다. 이전 뉴런이 활성화 됐었을 때는 1, 아닌 경우는 0으로 한다.\n",
    "\n",
    "> $\\frac{\\partial \\tilde{S}}{\\partial U}\\leftarrow S=\\left\\{\\begin{matrix} 1,\\ if\\ U>U_{thr}\\\\0,\\ \\ \\ \\ otherwise \\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky neuron model, overriding the backward pass with a custom function\n",
    "class LeakySurrogate(nn.Module):\n",
    "  def __init__(self, beta, threshold=1.0):\n",
    "      super(LeakySurrogate, self).__init__()\n",
    "\n",
    "      # initialize decay rate beta and threshold\n",
    "      self.beta = beta\n",
    "      self.threshold = threshold\n",
    "      self.spike_op = self.SpikeOperator.apply\n",
    "\n",
    "  # the forward function is called each time we call Leaky\n",
    "  def forward(self, input_, mem):\n",
    "    spk = self.spike_op((mem-self.threshold))  # call the Heaviside function\n",
    "    reset = (spk * self.threshold).detach()  # removes spike_op gradient from reset\n",
    "    mem = self.beta * mem + input_ - reset  # Eq (1)\n",
    "    return spk, mem\n",
    "\n",
    "  # Forward pass: Heaviside function\n",
    "  # Backward pass: Override Dirac Delta with the Spike itself\n",
    "  @staticmethod\n",
    "  class SpikeOperator(torch.autograd.Function):\n",
    "      @staticmethod\n",
    "      def forward(ctx, mem):\n",
    "          spk = (mem > 0).float() # Heaviside on the forward pass: Eq(2)\n",
    "          ctx.save_for_backward(spk)  # store the spike for use in the backward pass\n",
    "          return spk\n",
    "\n",
    "      @staticmethod\n",
    "      def backward(ctx, grad_output):\n",
    "          (spk,) = ctx.saved_tensors  # retrieve the spike\n",
    "          grad = grad_output * spk # scale the gradient by the spike: 1/0\n",
    "          return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lif1 = LeakySurrogate(beta=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snnTorch 에서는 위와 같은 역전파를 위한 *spike operator*가 모든 뉴런 모델 함수에 내장되어 있기 때문에, default로 사용하여도 문제가 없을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lif1 = snn.Leaky(beta=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Backpropagation through time\n",
    "\n",
    "> $\\frac{\\partial L}{\\partial W}=\\frac{\\partial L}{\\partial S}\\underbrace{\\frac{\\partial S}{\\partial U}}_{\\{0,\\infty\\}}\\frac{\\partial U}{\\partial I}\\frac{\\partial I}{\\partial W}$\n",
    "\n",
    "위 식은 단일 time-step 에서의 전개이다. 실제로는 모든 time-step에 걸쳐 해당 과정이 수행되어야 한다. 매 시간에서의 가중치에 의한 Loss의 기울기는 다음과 같이 적을 수 있을 것이다.\n",
    "\n",
    "> $\\frac{\\partial L}{\\partial W}=\\sum_{t}\\frac{\\partial L[t]}{\\partial W}=\\sum_{t}\\sum_{s\\leq t}\\frac{\\partial L[t]}{\\partial W[s]}\\frac{\\partial W[s]}{\\partial W}$\n",
    "\n",
    "이때, recurrent system에서 $W[s]$는 모든 $W$에 대해서 동일한 효과를 가진다. 즉, $\\frac{\\partial W[s]}{\\partial W}=1$ 이라고 할 수 있다. 이제 위 식을 다시 쓰면 아래와 같다.\n",
    "\n",
    "> $\\frac{\\partial L}{\\partial W}=\\sum_{t}\\sum_{s\\leq t}\\frac{\\partial L[t]}{\\partial W[s]}$\n",
    "\n",
    "전체적인 backprop 과정을 도식화하면 아래와 같다.\n",
    "\n",
    "![bptt](./images/bptt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setting up the loss & output decoding\n",
    "\n",
    "Classification 문제에서 SNN의 출력 spike를 해석하는 방식은 몇 가지가 있지만 가장 일반적인 것은 아래 두 가지이다.\n",
    "\n",
    "> **Rate coding**: 가장 많이 활성화된 뉴런\n",
    ">\n",
    "> **Latency coding**: 가장 먼저 활성화된 뉴런\n",
    ">\n",
    "> 을 예측 class로 한다.\n",
    "\n",
    "여기서는 **rate code** 방식을 사용한다. 출력 뉴런의 membrane potential에 대해서 softmax 함수를 적용하면 \n",
    "\n",
    "> $p_i[t]=\\frac{e^{U_i[t]}}{\\sum_{i=0}^{C}e^{U_i[t]}}$\n",
    "\n",
    "이렇게 나온 $p_i[t]$ 에 대해서 *cross-entropy error* 를 사용한다. 여기서 $C$ 는 전체 클래스의 개수이다.\n",
    "\n",
    "> $L_{CE}[t]=\\sum_{i=0}^Cy_ilog(p_i[t])$\n",
    "\n",
    "전체 time-step에 대해서 최종 loss는 아래와 같다고 할 수 있다.\n",
    "\n",
    "> $L_{CE}=\\sum_tL_{CE}[t]$\n",
    "\n",
    "이와 같은 방식은 한 예시일 뿐이고, *snn.functional* 을 통해 다양한 접근법을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Setting up the static MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader arguments\n",
    "batch_size = 128\n",
    "data_path  = './data'\n",
    "\n",
    "dtype  = torch.float\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28,28)), \n",
    "            transforms.Grayscale(), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test  = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs  = 28*28     # size of MNIST images\n",
    "num_hidden  = 1000\n",
    "num_outputs = 10        # num of MNIST classes\n",
    "\n",
    "# Temporal dynamics\n",
    "num_steps = 25\n",
    "beta      = 0.95\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.fc1  = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2  = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif1.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            \n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "            \n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "    \n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx    = output.sum(dim=0).max(1)\n",
    "    acc       = np.mean((targets==idx).detach().cpu().numpy())\n",
    "    \n",
    "    if train:\n",
    "        print(f'Train set accuracy for a single minibatch: {acc*100:.2f}%')\n",
    "    else:\n",
    "        print(f'Test set accuracy for a single minibatch: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 128, 10])\n"
     ]
    }
   ],
   "source": [
    "# Loss definition\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# One iteration of training\n",
    "data, targets = next(iter(train_loader))\n",
    "data    = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "print(mem_rec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing loss: 63.188\n",
      "Train set accuracy for a single minibatch: 7.03%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# Sum loss at every step\n",
    "for step in range(num_steps):\n",
    "    loss_val += loss(mem_rec[step], targets)\n",
    "    \n",
    "print(f'Traing loss: {loss_val.item():.3f}')\n",
    "print_batch_accuracy(data, targets, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 51.074\n",
      "Train set accuracy for a single minibatch: 42.19%\n"
     ]
    }
   ],
   "source": [
    "# clear previously stored gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# calculate the gradients\n",
    "loss_val.backward()\n",
    "\n",
    "# weight update\n",
    "optimizer.step()\n",
    "\n",
    "# calculate new network outputs using the same data\n",
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# sum loss at every step\n",
    "for step in range(num_steps):\n",
    "    loss_val += loss(mem_rec[step], targets)\n",
    "  \n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "print_batch_accuracy(data, targets, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('SNN-Torch-Practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "683a7b102b596254c395704dcf4840313938b4a008d6dc868d87bbb00d08873e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
